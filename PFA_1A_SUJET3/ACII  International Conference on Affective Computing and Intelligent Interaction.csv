Title
Seventh International Conference on Affective Computing and Intelligent Interaction  ACII 2017  San Antonio  TX  USA  October 23-26  2017.
Automated mood-aware engagement prediction.
How different identities affect cooperation.
What really matters - An information gain analysis of questions and reactions in automated PTSD screenings.
NAA: A multimodal database of negative affect and aggression.
Recognizing induced emotions of movie audiences: Are induced and perceived emotions the same?
Effects of valence and arousal on working memory performance in virtual reality gaming.
Computational model of idiosyncratic perception of others' emotions.
Comparing empathy perceived by interlocutors in multiparty conversation and external observers.
Towards modeling agent negotiators by analyzing human negotiation behavior.
Robust emotion recognition from low quality and low bit rate video: A deep learning approach.
Vocal markers of motor  cognitive  and depressive symptoms in Parkinson's disease.
Processing negative emotions through social communication: Multimodal database construction and analysis.
The effect of personality trait  age  and gender on the performance of automatic speech valence recognition.
Multiple users' emotion recognition: Improving performance by joint modeling of affective reactions.
Smiling from adolescence to old age: A large observational study.
Noninvasive estimation of cognitive status in mild traumatic brain injury using speech production and facial expression.
Local-global ranking for facial expression intensity estimation.
Discovering gender differences in facial emotion recognition via implicit behavioral cues.
Emotion detection using noninvasive low cost sensors.
Assessing personality through objective behavioral sensing.
Comparing models for gesture recognition of children's bullying behaviors.
Evaluating effectiveness of smartphone typing as an indicator of user emotion.
Stress measurement from tongue color imaging.
RankTrace: Relative and unbounded affect annotation.
Computational analysis of valence and arousal in virtual reality gaming using lower arm electromyograms.
Modeling doctor-patient communication with affective text analysis.
Response to name: A dataset and a multimodal machine learning framework towards autism study.
Toward affect-sensitive virtual human tutors: The influence of facial expressions on learning and emotion.
Segment-based speech emotion recognition using recurrent neural networks.
Emo-soundscapes: A dataset for soundscape emotion recognition.
Multimodal autoencoder: A deep learning approach to filling in missing sensor data and enabling better mood prediction.
Hand2Face: Automatic synthesis and recognition of hand over face occlusions.
Automatic action unit detection in infants using convolutional neural network.
Are you stressed? Your eyes and the mouse can tell.
Manual and automatic measures confirm - Intranasal oxytocin increases facial expressivity.
Weighted geodesic flow kernel for interpersonal mutual influence modeling and emotion recognition in dyadic interactions.
Perceptual enhancement of emotional mocap head motion: An experimental study.
The ordinal nature of emotions.
Improved facial expression recognition method based on ROI deep convolutional neutral network.
Speech emotion recognition in noisy and reverberant environments.
Exploring sparse representation measures of physiological synchrony for romantic couples.
Emotional responses of vibrotactile-thermal stimuli: Effects of constant-temperature thermal stimuli.
The ABC of MOOCs: Affect and its inter-play with behavior and cognition.
Affect recognition in an interactive gaming environment using eye tracking.
NNIME: The NTHU-NTUA Chinese interactive multimodal emotion corpus.
Aggression recognition using overlapping speech.
Emotion-augmented machine learning: Overview of an emerging domain.
Embedding stacked bottleneck vocal features in a LSTM architecture for automatic pain level classification during emergency triage.
Automatic emotional spoken language text corpus construction from written dialogs in fictions.
Objective assessment of depressive symptoms with machine learning and wearable sensors data.
Towards general models of player affect.
CAST a database: Rapid targeted large-scale big data acquisition via small-world modelling of social media platforms.
Designing opportune stress intervention delivery timing using multi-modal data.
Toward automatic detection of acute stress: Relevant nonverbal behaviors and impact of personality traits.
Exploring affection-oriented virtual pet game design strategies in VR attachment  motivations and expectations of users of pet games.
Photorealistic facial expression synthesis by the conditional difference adversarial autoencoder.
A bootstrapped multi-view weighted Kernel fusion framework for cross-corpus integration of multimodal emotion recognition.
Learning spectro-temporal features with 3D CNNs for speech emotion recognition.
Multimodal classification of driver glance.
Facial action units detection under pose variations using deep regions learning.
Facial action unit intensity estimation and feature relevance visualization with random regression forests.
Exploring moral conflicts in speech: Multidisciplinary analysis of affect and stress.
GIFGIF+: Collecting emotional animated GIFs with clustered multi-task learning.
Formulating emotion perception as a probabilistic model with application to categorical emotion classification.
A taxonomy of mood research and its applications in computer science.
Refactoring facial expressions: An automatic analysis of natural occurring facial expressions in iterative social dilemma.
Predicting speaker recognition reliability by considering emotional content.
An investigation into three visual characteristics of complex scenes that evoke human emotion.
Decoding the perception of sincerity in written dialogues.
DeepBreath: Deep learning of breathing patterns for automatic stress recognition using low-cost thermal imaging in unconstrained settings.
Comparing virtual reality with computer monitors as rating environments for affective dimensions in social interactions.
Toward active and unobtrusive engagement assessment of distance learners.
Grounded emotions.
DCNN and DNN based multi-modal depression recognition.
Visual attention in schizophrenia: Eye contact and gaze aversion during clinical interactions.
Heart rate estimation from facial videos for depression analysis.
Automated video interview judgment on a large-sized corpus collected online.
Modeling variable length phoneme sequences - A step towards linguistic information for speech emotion recognition in wider world.
An exploratory study of population differences based on massive database of physiological responses to music.
Investigating gender differences in temporal dynamics during an iterated social dilemma: An automatic analysis using networks.
Spontaneous and posed smile recognition based on spatial and temporal patterns of facial EMG.
Using natural language processing tools to develop complex models of student engagement.
The dance of emotion: Demonstrating ubiquitous understanding of human motion and emotion in support of human computer interaction.
CNN based 3D facial expression recognition using masking and landmark features.
Reminiscence therapy improvement using emotional information.
Perceived emotion from images through deep neural networks.
Avatar and participant gender differences in the perception of uncanniness of virtual humans.
Building a generalized model for multi-lingual vocal emotion conversion.
Learning based visual engagement and self-efficacy.
Automatic personality assessment in the wild.
Wear your heart on your sleeve: Visible psychophysiology for contextualized relaxation.
Automated mental stress recognition through mobile thermal imaging.
Nonverbal conversation expressions processing for human-agent interactions.
Dynamic emotion transitions based on emotion hysteresis.
Towards more meaningful interactive narrative with intelligent affective characters.
Temporal patterns of facial expression in deceptive and honest communication.
